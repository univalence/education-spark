{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617242c4-f978-4e21-b194-c43b6434db97",
   "metadata": {},
   "source": [
    "# Fonction d'ordre supérieur\n",
    "\n",
    "Les fonctions d'ordre supérieur dans Spark SQL sont des fonctions qui prennent d'autres fonctions comme arguments ou qui retournent des fonctions. Ces fonctions permettent d'effectuer des opérations complexes sur des données structurées telles que des tableaux ou des structures dans Spark SQL.\n",
    "\n",
    "À partir de la version 2.4, Spark SQL a introduit plusieurs fonctions d'ordre supérieur pour travailler avec des données complexes. Voici quelques exemples de fonctions d'ordre supérieur couramment utilisées dans Spark SQL :\n",
    "\n",
    "* **transform** : Applique une fonction donnée à chaque élément d'un tableau et retourne un nouveau tableau avec les résultats.<br />Syntaxe : `transform(array, function)`.\n",
    "* **filter** : Retourne un nouveau tableau contenant les éléments qui satisfont la condition spécifiée par la fonction donnée.<br />Syntaxe : `filter(array, function)`.\n",
    "* **exists** : Vérifie si au moins un élément d'un tableau satisfait la condition spécifiée par la fonction donnée.<br />Syntaxe : `exists(array, function)`.\n",
    "* **aggregate** : Agrège les éléments d'un tableau à l'aide d'une fonction d'agrégation et d'une valeur initiale.<br />Syntaxe : `aggregate(array, initial_value, merge_function[, finish_function])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32428cf8-b4d6-4701-9c53-c11af78affe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7dfa8-24a3-4426-81ab-3138506b1d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.3.2`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.2`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Avoid disturbing logs\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a591d47-3e95-415e-9177-90eb995a2e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    // L'appel ci-dessous sert à donner un nom à votre application\n",
    "    // Ce apparaîtra notamment dans la Spark UI\n",
    "    .appName(\"Sales Analysis - SparkSQL\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Ce script fournit que élément supplémentaires pour rendre l'affichage plus confortable\n",
    "import $file.^.internal.spark_helper, spark_helper._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ad563-d9d0-4d2c-b99f-6ad1513bdbf2",
   "metadata": {},
   "source": [
    "## Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4f237-e002-4ae9-9220-0f064ed91c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // demande à Spark SQL de tenter de déterminer le type des colonnes\n",
    "    .schema(\"id STRING, client STRING, timestamp TIMESTAMP, product STRING, price DOUBLE\")\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "import java.sql.Timestamp\n",
    "\n",
    "case class Order(\n",
    "  id:        String,\n",
    "  clientId:  String,\n",
    "  timestamp: Timestamp,\n",
    "  product:   String,\n",
    "  price:     Double\n",
    ")\n",
    "\n",
    "val orders: Dataset[Order] =\n",
    "  dataframe\n",
    "    .withColumnRenamed(\"client\", \"clientId\")\n",
    "    .as[Order]\n",
    "\n",
    "orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e36cc8-18f2-45ce-a862-277af699a99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val pricesByDay =\n",
    "  orders\n",
    "    .groupBy(to_date($\"timestamp\").as(\"date\"))\n",
    "    .agg(collect_list($\"price\").as(\"price\"))\n",
    "\n",
    "pricesByDay.showHTML(truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd766915-2ef0-4e82-b6b4-994b1a193176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pricesByDay\n",
    "  .select(\n",
    "      $\"date\",\n",
    "      transform($\"price\", _ => lit(1)).as(\"one\"),\n",
    "      aggregate($\"price\", lit(0.0), _ + _).as(\"total\")\n",
    "  )\n",
    "\n",
    ".showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784baba-b688-4899-a33c-13d7f2e40568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  date,\n",
    "  transform(price, p -> 1) AS one,\n",
    "  aggregate(price, CAST(0.0 AS double), (p1, p2) -> p1 + p2) AS total\n",
    "FROM (\n",
    "  SELECT\n",
    "    to_date(timestamp) AS date,\n",
    "    collect_list(price) AS price\n",
    "  FROM orders\n",
    "  GROUP BY date\n",
    ")\n",
    "\"\"\").showHTML(limit=10, truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e918f-d39c-4adb-94c9-dfb8cab62e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496c090-7afe-404d-8e8c-438d45a81fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
