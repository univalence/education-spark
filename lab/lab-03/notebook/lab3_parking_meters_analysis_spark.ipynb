{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed computation\n",
    "## ESIPE — INFO 3 — Option Logiciel\n",
    "<style type=\"text/css\">\n",
    "    .question {\n",
    "        background-color: yellow;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 : Parking meters devices analysis with Apache Spark\n",
    "\n",
    "In this lab, we will analyse the parking meters of Paris for the year 2014. The dataset is composed in two parts:\n",
    "* The parking meter devices\n",
    "* The transactions\n",
    "\n",
    "All the data needed for this evaluation are located in the directory `data`.\n",
    "\n",
    "This notebook is divided in 5 parts:\n",
    "* PART 1: Initiate the environment (/1)\n",
    "* PART 2: Get and analyse the device dataset (/3)\n",
    "* PART 3: Get and analyse the transaction dataset (/7)\n",
    "* PART 4: Joining devices and transactions (/5)\n",
    "* PART 5: Analytics on a map (/4)\n",
    "\n",
    "All questions are highlight in <span style=\"background-color: yellow\">yellow</span>. They have to be answered using Spark Core / SQL / ML features. Indicative rating is given for each question (total / 20).\n",
    "\n",
    "During this evaluation, you can access to any support including internet, course lectures and labs. the use of online messaging and drives are not permitted during this session.\n",
    "\n",
    "<span style=\"background-color: #ffbbaa;\">**Do not forget to oftenly save your whole notebook.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.3.1`\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Initiate the environment (/1)\n",
    "To do our analysis, we will use Spark SQL.\n",
    "\n",
    "<span style=\"background-color: yellow;\">Create a NotebookSparkSession and assign it to the variable `spark`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = ???\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "println(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, the Spark UI interface is available at http://localhost:4040/ or  http://localhost:4041/.\n",
    "\n",
    "We will need also many Spark SQL tools. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Get and analyse the device dataset (/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parking meter files (/0.5)\n",
    "The parking meter devices are stored in a JSON file of 4.5MB.\n",
    "\n",
    "<span style=\"background-color: yellow;\">Read file `data/horodateurs-mobiliers.json` and store it in a variable named `raw_parkmeters`. Display its content by using the method `.show()`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// read \"data/horodateurs-mobiliers.json\"\n",
    "val raw_parkmeters = ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display schema (/0.5)\n",
    "The file comes with nested records. We will need to simplify its structure.\n",
    "\n",
    "To understand its structure, <span style=\"background-color: yellow;\">display the schema of `raw_parkmeters`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_parkmeters.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify dataframe (/2)\n",
    "We are here interested only on those fields:\n",
    "* `numhoro`: parking meter number (it must be renamed to `parkmeter_id`)\n",
    "* `arrondt`: district number in Paris (it must be renamed to `district`)\n",
    "* `regime`: pricing mode (MIX = includes specific rule for inhabitants (_résident_), ROT = everyone follows the same rules - it must be renamed `type`)\n",
    "* `zoneres`: residential area (it must be renamed to `area`)\n",
    "\n",
    "<span style=\"background-color: yellow;\">Create a new dataframe from `raw_parkmeters` named `parkmeters`, that includes only the fields shown above.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val parkmeters = raw_parkmeters.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Get and analyse the transaction dataset (/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the files (/0.5)\n",
    "<span style=\"background-color: yellow;\">Read all the files in `data/horodateurs-transactions-de-paiement` directory in a single command to create a dataframe named `raw_transactions`.</span>\n",
    "\n",
    "Pay attention to the fact that there is a header in the files and that the semi-colon (`;`) is used as a field delimiter. For the last one, we will use the option `.option(\"delimiter\", \";\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val raw_transactions = spark.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the content (/0.5)\n",
    "<span style=\"background-color: yellow;\">Display its content by using the method `.show()`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: here `usager` are the users of parking meters. They can be Résident (or `R�sident`), if they are inhabitants. They can be `Rotatif`, if they are considered as occasional visitors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The schema (/0.5)\n",
    "<span style=\"background-color: yellow;\">Now, display the schema of the dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning (/5)\n",
    "The dataset comes with some inconveniences:\n",
    "* Everything is a string in this schema\n",
    "* Some columns have name with strange characters\n",
    "* Numbers are in French format\n",
    "* Timestamps are in french format too\n",
    "\n",
    "To improve the dataset, we will provide two functions:\n",
    "* `toDouble` that takes a column representing a number, replace \",\" by \".\" and cast it into DoubleType (you will need the Spark SQL function `translate`)\n",
    "* `toTimestamp` that takes a column representing a timestamp with the format `\"dd/MM/yyyy HH:mm:ss\"` and convert it Unix timestamp (you will need the Spark SQL function `unix_timestamp` with two parameters). A Unix timestamp is in seconds.\n",
    "\n",
    "But first, let run the cell below, that creates a function to simplify the writing of unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(function : (Column => Column), de : DataFrame): Unit = {\n",
    "        val text_df = de.toDF(\"data\", \"expected\")\n",
    "        val result = text_df\n",
    "          .withColumn(\"result\", function(col(\"data\")))\n",
    "          .withColumn(\"succeed\", col(\"expected\") === col(\"result\"))\n",
    "        result.show()\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ToDouble function (/1)\n",
    "<span style=\"background-color: yellow;\">Complete the function `toDouble`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def toDouble(column: Column): Column = ???\n",
    "\n",
    "// Unit test\n",
    "val data_expected = Seq(\n",
    "    (\"1,0\", 1.0),\n",
    "    (\"3,4\", 3.4),\n",
    "    (\"0,65\", 0.65)\n",
    ").toDF()\n",
    "test_function(toDouble, data_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ToTimestamp function (/1)\n",
    "<span style=\"background-color: yellow;\">Complete the function `toTimestamp`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def toTimestamp(column: Column): Column = ???\n",
    "\n",
    "// Unit tests\n",
    "val data_expected = Seq(\n",
    "    (\"31/01/2014 15:09:33\", 1391180973),\n",
    "    (\"24/01/2014 13:56:24\", 1390571784),\n",
    "    (\"26/01/2014 19:21:09\", 1390764069)\n",
    ").toDF\n",
    "test_function(toTimestamp, data_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning process (/3)\n",
    "Now do the cleaning:\n",
    "* `horodateur` needs to be renamed into `parkmeter_id`\n",
    "* `montant carte` needs to be converted into number and renamed `amount`\n",
    "* `début stationnement` needs to be converted into timestamp and renamed `parking_start`\n",
    "* `fin stationnement` needs to be converted into timestamp and renamed `parking_end`\n",
    "\n",
    "You will also add a column `duration`, that is the result of the difference between `parking_start` and `parking_end`. Make sure that `duration` is in hours, knowing that `parking_start` and `parking_end` are in seconds.\n",
    "\n",
    "We only want transactions for users marked as `Rotatif`.\n",
    "\n",
    "<span style=\"background-color: yellow;\">Starts from `raw_transactions` and apply all the cleaning rules seen above to create the dataframe `transactions`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val transactions = raw_transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow;\">Use `.show()` method to display the content of `transactions`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of records (/0.5)\n",
    "\n",
    "<span style=\"background-color: yellow;\">Display the number of records in `transactions`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Joining devices and transactions (/5)\n",
    "Now that we have the devices location and the transactions, we can merge those two datasets and do different analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining (/2)\n",
    "<span style=\"background-color: yellow;\">Create a dataframe named `parkmeter_transactions`, that joins the dataframes `parkmeters` and `transactions`.</span>\n",
    "\n",
    "* Keep only those columns: `\"parkmeter_id\", \"district\", \"area\", \"duration\", \"parking_start\", \"parking_end\", \"amount\"`\n",
    "* Beware! some columns are defined both in `transactions` and in `parkmeters`. Depending, on the way you reference a column, it can lead Spark to confusion and thus a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val parkmeter_transactions = ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the join (/1)\n",
    "Before going further, due to the size of the data, the relative heaviness of the processing, and the weakness of the machine you are working on, it is preferable to store data in a Parquet file first.\n",
    "\n",
    "Once written, this file will be used as a checkpoint. So, **if something goes wrong in your notebook, you can start again from the read of the parquet file below.**\n",
    "\n",
    "<span style=\"background-color: yellow;\">Store the `parkmeter_transactions` dataframe in the Parquet file `parkmeter_transactions.parquet`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkmeter_transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow;\">Now load the file in `parkmeter_transactions`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parkmeter_transactions = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First analysis of parkmeter_transactions (/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do an analysis of dataframe `parkmeter_transactions`. For that we will use the method `.describe()` available on dataframes. `.describe()` returns a dataframe with stats on the different columns.\n",
    "\n",
    "<span style=\"background-color: yellow;\">Use `.describe()` on `parkmeter_transactions` and display its result.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkmeter_transactions.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkmeter_transactions.select($\"parking_end\").orderBy(\"parking_end\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` row shows the number of non-null elements for each column.\n",
    "\n",
    "What can you identify from the result of `.describe()`?\n",
    "\n",
    "<span style=\"background-color: yellow;\">Update `parkmeter_transactions` to remove rows with undesirable values.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parkmeter_transactions_updated = parkmeter_transactions.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: Analytics (/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of transactions (/2)\n",
    "<span style=\"background-color: yellow;\">Find the number of transactions per district on the map of Paris, the columns must be \"district\" and \"count_transactions\".</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val count_transactions = ???\n",
    "\n",
    "count_transactions.orderBy(\"district\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow;\">Find the number of transactions per area on the map of Paris, the columns must be \"area\" and \"count_transactions\".</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val count_transactions = ???\n",
    "\n",
    "count_transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average transaction amount (/2)\n",
    "<span style=\"background-color: yellow;\">Find the average transaction amount per district in Paris, the columns must be \"district\" and \"avg_amount\".</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avg_amount = parkmeter_transactions.???\n",
    "\n",
    "avg_amount.orderBy(\"district\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow;\">Find the average transaction amount per area in Paris, the columns must be \"area\" and \"avg_amount\".</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avg_amount = parkmeter_transactions.???\n",
    "\n",
    "avg_amount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
